{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to run TbD-net through test data on the CLEVR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from tbd.module_net import load_tbd_net\n",
    "from utils.clevr import load_vocab\n",
    "from utils.generate_programs import load_program_generator, generate_programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model we want to produce test answers for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = Path('data/vocab.json')\n",
    "model_path = Path('models/clevr-reg-hres.pt')\n",
    "tbd_net = load_tbd_net(model_path, load_vocab(vocab_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate, we first need to generate programs from the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'data\\test\\test_questions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-3-1abc5b88ec5e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mprogram_generator\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mload_program_generator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mPath\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'models/program_generator.pt'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m generate_programs(Path('data/test/test_questions.h5'), program_generator, \n\u001B[0m\u001B[0;32m      3\u001B[0m                   dest_dir=Path('data/test/'), batch_size=128)\n",
      "\u001B[1;32m~\\PycharmProjects\\tbd-nets\\utils\\generate_programs.py\u001B[0m in \u001B[0;36mgenerate_programs\u001B[1;34m(h5_file, program_generator, dest_dir, batch_size)\u001B[0m\n\u001B[0;32m    236\u001B[0m     \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    237\u001B[0m     \"\"\"\n\u001B[1;32m--> 238\u001B[1;33m     \u001B[1;32mwith\u001B[0m \u001B[0mh5py\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mFile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mh5_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mquestions_h5\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    239\u001B[0m         \u001B[0mquestions\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mquestions_h5\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'questions'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    240\u001B[0m         \u001B[0mimage_indices\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mquestions_h5\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'image_idxs'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\rapid\\lib\\site-packages\\h5py\\_hl\\files.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, **kwds)\u001B[0m\n\u001B[0;32m    422\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mphil\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    423\u001B[0m                 \u001B[0mfapl\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmake_fapl\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdriver\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlibver\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrdcc_nslots\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrdcc_nbytes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrdcc_w0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 424\u001B[1;33m                 fid = make_fid(name, mode, userblock_size,\n\u001B[0m\u001B[0;32m    425\u001B[0m                                fapl, fcpl=make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001B[0;32m    426\u001B[0m                                fs_persist=fs_persist, fs_threshold=fs_threshold),\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\rapid\\lib\\site-packages\\h5py\\_hl\\files.py\u001B[0m in \u001B[0;36mmake_fid\u001B[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001B[0m\n\u001B[0;32m    188\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mswmr\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mswmr_support\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    189\u001B[0m             \u001B[0mflags\u001B[0m \u001B[1;33m|=\u001B[0m \u001B[0mh5f\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mACC_SWMR_READ\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 190\u001B[1;33m         \u001B[0mfid\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mh5f\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mflags\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfapl\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mfapl\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    191\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mmode\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'r+'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    192\u001B[0m         \u001B[0mfid\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mh5f\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mh5f\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mACC_RDWR\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfapl\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mfapl\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mh5py\\_objects.pyx\u001B[0m in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mh5py\\_objects.pyx\u001B[0m in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mh5py\\h5f.pyx\u001B[0m in \u001B[0;36mh5py.h5f.open\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mOSError\u001B[0m: Unable to open file (unable to open file: name = 'data\\test\\test_questions.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "program_generator = load_program_generator(Path('models/program_generator.pt'))\n",
    "generate_programs(Path('data/test/test_questions.h5'), program_generator, \n",
    "                  dest_dir=Path('data/test/'), batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test features that we've extracted and the the questions, image indices, and programs we just generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_np_features = True\n",
    "if use_np_features:\n",
    "    features = np.load(str(Path('data/test/test_features.npy')), mmap_mode='r')\n",
    "else:\n",
    "    features = h5py.File(Path('data/test/test_features.h5'))['features']\n",
    "\n",
    "question_np = np.load(Path('data/test/test_questions.npy'))\n",
    "image_idx_np = np.load(Path('data/test/test_image_idxs.npy'))\n",
    "programs_np = np.load(Path('data/test/test_programs.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mapping from our model output to answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = ['blue', 'brown', 'cyan', 'gray', 'green', 'purple', 'red', 'yellow',\n",
    "           'cube', 'cylinder', 'sphere',\n",
    "           'large', 'small',\n",
    "           'metal', 'rubber',\n",
    "           'no', 'yes',\n",
    "           '0', '1', '10', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "pred_idx_to_token = dict(zip(range(len(answers)), answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convenience function for writing predictions to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('predicted_answers.txt', 'w')\n",
    "def write_preds(preds):\n",
    "    for pred in preds:\n",
    "        f.write(pred)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a handle to the device we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run through all of the questions, produce a prediction, and write that predicted answer to the text file we opened above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "for batch in range(0, len(programs_np), batch_size):\n",
    "    image_idx = image_idx_np[batch:batch+batch_size]\n",
    "    programs = torch.LongTensor(programs_np[batch:batch+batch_size]).to(device)\n",
    "    \n",
    "    if use_np_features:\n",
    "        feats = torch.FloatTensor(np.asarray(features[image_idx])).to(device)\n",
    "    else:\n",
    "        # Using HDF5 files requires some overhead due to constraints on how those may\n",
    "        # be accessed. We cannot index into the file using a numpy array. We also cannot \n",
    "        # access the same element multiple times (e.g. we cannot index into an h5py.File \n",
    "        # with [1,1,1]) because we are constrained to increasing sequences\n",
    "        feats = []\n",
    "        for idx in image_idx:\n",
    "            feats.append(np.asarray(features[idx]))\n",
    "        feats = torch.FloatTensor(np.asarray(feats)).to(device)\n",
    "\n",
    "    outputs = tbd_net(feats, programs)\n",
    "    _, preds = outputs.max(1)\n",
    "    preds = [pred_idx_to_token[pred] for pred in preds.detach().to('cpu').numpy()]\n",
    "    write_preds(preds)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapid",
   "language": "python",
   "name": "rapid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}